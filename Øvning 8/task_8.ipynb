{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "\n",
    "class QLearningCartPole:\n",
    "    \"\"\"\n",
    "    Q-learning Agent for the CartPole problem using OpenAI Gym.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, buckets=(2, 2, 6, 12), num_episodes=1000, min_lr=0.1, min_epsilon=0.1, discount=1.0, decay=25):\n",
    "        \"\"\"\n",
    "        Initialize the Q-learning agent.\n",
    "\n",
    "        Parameters:\n",
    "        - buckets: Tuple defining the discretization for each state dimension.\n",
    "        - num_episodes: Number of episodes for training.\n",
    "        - min_lr: Minimum learning rate.\n",
    "        - min_epsilon: Minimum exploration rate.\n",
    "        - discount: Discount factor for future rewards.\n",
    "        - decay: Rate at which learning and exploration rates decay.\n",
    "        \"\"\"\n",
    "\n",
    "        # Setting the agent parameters\n",
    "        self.buckets = buckets\n",
    "        self.num_episodes = num_episodes\n",
    "        self.min_lr = min_lr\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.discount = discount\n",
    "        self.decay = decay\n",
    "        self.env = env\n",
    "\n",
    "        # Define bounds for discretizing the state space\n",
    "        self.upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50) / 1.]\n",
    "        self.lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], - math.radians(50) / 1.]\n",
    "\n",
    "        # Initialize the Q-table with zeros\n",
    "        self.Q_table = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "\n",
    "    def discretize_state(self, obs):\n",
    "        state, _ = obs \n",
    "        _, _, angle, angle_velocity = state\n",
    "        est = KBinsDiscretizer(n_bins=self.buckets[2:], encode='ordinal', strategy='uniform')\n",
    "        est.fit([self.lower_bounds[2:], self.upper_bounds[2:]])\n",
    "        return tuple(map(int, est.transform([[angle, angle_velocity]])[0]))\n",
    "\n",
    "    \"\"\"\n",
    "    OLD METHOD\n",
    "\n",
    "    def discretize_state(self, obs):\n",
    "        discretized = list()\n",
    "        for i in range(len(obs)):\n",
    "            scaling = (obs[i] + abs(self.lower_bounds[i])) / (self.upper_bounds[i] - self.lower_bounds[i])\n",
    "\n",
    "            print((self.buckets[i] - 1) * scaling)\n",
    "\n",
    "            new_obs = float(np.round((self.buckets[i] - 1) * scaling))\n",
    "            new_obs = min(self.buckets[i] - 1, max(0, new_obs))\n",
    "            discretized.append(new_obs)\n",
    "        return tuple(discretized)\n",
    "    \"\"\"\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Choose an action based on the current state and epsilon (exploration rate).\n",
    "\n",
    "        Parameters:\n",
    "        - state: The current discretized state.\n",
    "        - epsilon: Current exploration rate.\n",
    "\n",
    "        Returns:\n",
    "        - The chosen action.\n",
    "        \"\"\"\n",
    "        if np.random.random() <= epsilon:\n",
    "            return self.env.action_space.sample() \n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state])\n",
    "\n",
    "    def update_q(self, state, action, reward, new_state):\n",
    "        \"\"\"\n",
    "        Update Q-value for the given state and action.\n",
    "\n",
    "        Parameters:\n",
    "        - state: The current discretized state.\n",
    "        - action: The taken action.\n",
    "        - reward: The received reward.\n",
    "        - new_state: The new state after taking the action.\n",
    "        \"\"\"\n",
    "        self.Q_table[state][action] += self.learning_rate * (reward + self.discount * np.max(self.Q_table[new_state]) - self.Q_table[state][action])\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        \"\"\"\n",
    "        Compute epsilon (exploration rate) based on episode number.\n",
    "\n",
    "        Parameters:\n",
    "        - t: Current episode number.\n",
    "\n",
    "        Returns:\n",
    "        - The exploration rate (epsilon).\n",
    "        \"\"\"\n",
    "        return max(self.min_epsilon, min(1., 1. - np.log10((t + 1) / self.decay)))\n",
    "\n",
    "    def get_learning_rate(self, t):\n",
    "        \"\"\"\n",
    "        Compute learning rate based on episode number.\n",
    "\n",
    "        Parameters:\n",
    "        - t: Current episode number.\n",
    "\n",
    "        Returns:\n",
    "        - The learning rate.\n",
    "        \"\"\"\n",
    "        return max(self.min_lr, min(1., 1. - np.log10((t + 1) / self.decay)))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the agent using Q-learning.\n",
    "        \"\"\"\n",
    "        for e in range(self.num_episodes):\n",
    "            current_state = self.discretize_state(self.env.reset())\n",
    "\n",
    "            self.learning_rate = self.get_learning_rate(e)\n",
    "            self.epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = self.choose_action(current_state, self.epsilon)\n",
    "\n",
    "                print(self.env.step(action))\n",
    "                items, dtype, _, _, _ = self.env.step(action)\n",
    "                print(items)\n",
    "\n",
    "                obs, reward, done, _ = items\n",
    "                new_state = self.discretize_state(obs)\n",
    "                self.update_q(current_state, action, reward, new_state)\n",
    "                current_state = new_state\n",
    "\n",
    "        print(\"Training finished.\")\n",
    "        self.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.02920011,  0.21637517,  0.03990792, -0.25328717], dtype=float32), 1.0, False, False, {})\n",
      "[ 0.03352761  0.41090524  0.03484217 -0.5331201 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\Desktop\\NTNU Ting\\5. Semester\\MachineLearning\\Øvninger\\MLenviroment\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "c:\\Users\\chris\\Desktop\\NTNU Ting\\5. Semester\\MachineLearning\\Øvninger\\MLenviroment\\lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:215: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable numpy.float32 object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chris\\Desktop\\NTNU Ting\\5. Semester\\MachineLearning\\Øvninger\\Øvning 8\\task_8.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/NTNU%20Ting/5.%20Semester/MachineLearning/%C3%98vninger/%C3%98vning%208/task_8.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mCartPole-v1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/NTNU%20Ting/5.%20Semester/MachineLearning/%C3%98vninger/%C3%98vning%208/task_8.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m QLearningCartPole(env)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/NTNU%20Ting/5.%20Semester/MachineLearning/%C3%98vninger/%C3%98vning%208/task_8.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m scores \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain()\n",
      "\u001b[1;32mc:\\Users\\chris\\Desktop\\NTNU Ting\\5. Semester\\MachineLearning\\Øvninger\\Øvning 8\\task_8.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/NTNU%20Ting/5.%20Semester/MachineLearning/%C3%98vninger/%C3%98vning%208/task_8.ipynb#X16sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39mprint\u001b[39m(items)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/NTNU%20Ting/5.%20Semester/MachineLearning/%C3%98vninger/%C3%98vning%208/task_8.ipynb#X16sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m obs, reward, done, _ \u001b[39m=\u001b[39m items\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/NTNU%20Ting/5.%20Semester/MachineLearning/%C3%98vninger/%C3%98vning%208/task_8.ipynb#X16sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m new_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdiscretize_state(obs)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/NTNU%20Ting/5.%20Semester/MachineLearning/%C3%98vninger/%C3%98vning%208/task_8.ipynb#X16sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_q(current_state, action, reward, new_state)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/NTNU%20Ting/5.%20Semester/MachineLearning/%C3%98vninger/%C3%98vning%208/task_8.ipynb#X16sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m current_state \u001b[39m=\u001b[39m new_state\n",
      "\u001b[1;32mc:\\Users\\chris\\Desktop\\NTNU Ting\\5. Semester\\MachineLearning\\Øvninger\\Øvning 8\\task_8.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/NTNU%20Ting/5.%20Semester/MachineLearning/%C3%98vninger/%C3%98vning%208/task_8.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdiscretize_state\u001b[39m(\u001b[39mself\u001b[39m, obs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/NTNU%20Ting/5.%20Semester/MachineLearning/%C3%98vninger/%C3%98vning%208/task_8.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     state, _ \u001b[39m=\u001b[39m obs \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/NTNU%20Ting/5.%20Semester/MachineLearning/%C3%98vninger/%C3%98vning%208/task_8.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     _, _, angle, angle_velocity \u001b[39m=\u001b[39m state\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/NTNU%20Ting/5.%20Semester/MachineLearning/%C3%98vninger/%C3%98vning%208/task_8.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     est \u001b[39m=\u001b[39m KBinsDiscretizer(n_bins\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuckets[\u001b[39m2\u001b[39m:], encode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mordinal\u001b[39m\u001b[39m'\u001b[39m, strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39muniform\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable numpy.float32 object"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "model = QLearningCartPole(env)\n",
    "scores = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chris\\Desktop\\NTNU Ting\\5. Semester\\MachineLearning\\Øvninger\\Øvning 8\\task_8.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/NTNU%20Ting/5.%20Semester/MachineLearning/%C3%98vninger/%C3%98vning%208/task_8.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(scores,  c\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mblue\u001b[39m\u001b[39m'\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/NTNU%20Ting/5.%20Semester/MachineLearning/%C3%98vninger/%C3%98vning%208/task_8.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mlegend()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(scores,  c='blue', label='epochs')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenviroment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
